{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-13T05:17:50.472093Z",
     "iopub.status.busy": "2022-12-13T05:17:50.471684Z",
     "iopub.status.idle": "2022-12-13T05:17:53.012156Z",
     "shell.execute_reply": "2022-12-13T05:17:53.010508Z",
     "shell.execute_reply.started": "2022-12-13T05:17:50.472059Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-13T05:17:55.819951Z",
     "iopub.status.busy": "2022-12-13T05:17:55.819015Z",
     "iopub.status.idle": "2022-12-13T05:18:18.600139Z",
     "shell.execute_reply": "2022-12-13T05:18:18.598799Z",
     "shell.execute_reply.started": "2022-12-13T05:17:55.819888Z"
    }
   },
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Data_set(Dataset):\n",
    "    def __init__(self, file):\n",
    "        self.dataset = pd.read_csv(file, sep='\\t')\n",
    "#         self.dataset = load_from_disk('./ChnSentiCorp')[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        text = self.dataset['text_a'][i]\n",
    "        label = self.dataset['label'][i]\n",
    "        \n",
    "        return text, label\n",
    "    \n",
    "train_data = Data_set('D:\\\\jupyter\\\\Chinese\\\\train.tsv')\n",
    "dev_data = Data_set('D:\\\\jupyter\\\\Chinese\\\\dev.tsv')\n",
    "test_data = Data_set('D:\\\\jupyter\\\\Chinese\\\\test.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ss_data(data):\n",
    "    t_data = [i[0] for i in data]\n",
    "    t_label = [i[1] for i in data]\n",
    "    data_token = tokenizer.batch_encode_plus(batch_text_or_text_pairs=t_data, truncation = True, padding ='max_length',\n",
    "                                             max_length = 512, return_tensors ='pt', return_length=True)\n",
    "    #input_ids:编码之后的数字\n",
    "    #attention_mask:是补零的位置是0,其他位置是1\n",
    "    input_ids = data_token['input_ids']\n",
    "    attention_mask = data_token['attention_mask']\n",
    "#     token_type_ids = data_token['token_type_ids'] # Roberta 不需要这个\n",
    "    labels = torch.LongTensor(t_label)\n",
    "    \n",
    "    return input_ids, attention_mask, labels\n",
    "\n",
    "\n",
    "train_data_loader = DataLoader(train_data, batch_size = 16,collate_fn = ss_data, shuffle=True, drop_last=False)\n",
    "dev_data_loader = DataLoader(dev_data, batch_size = 16, collate_fn = ss_data, shuffle=True, drop_last=False)\n",
    "test_data_loader = DataLoader(test_data, batch_size = 16, collate_fn = ss_data, shuffle=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentiment(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Sentiment, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "        self.out = nn.Linear(768, 3)\n",
    "    \n",
    "    def forward(self, input_ids,  attention_mask):\n",
    "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        out1 = self.out(output[1])\n",
    "        out = out1.softmax(dim=1)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sentiment()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "epochs = 1\n",
    "validation = True\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "total_steps = len(train_data_loader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps=0,num_training_steps=total_steps)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "model.train()\n",
    "for epcoh in range(epochs):\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    for data in tqdm(train_data_loader):\n",
    "        data = [i.to(device) for i in data]\n",
    "        outputs = model(input_ids=data[0], attention_mask=data[1])\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        target = data[2]\n",
    "        loss = loss_fn(outputs,target)\n",
    "\n",
    "        correct_predictions += torch.sum(preds == target)\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        train_acc = correct_predictions.double() / len(train_data)\n",
    "        train_loss = np.mean(losses)\n",
    "        \n",
    "        \n",
    "    if validation:\n",
    "        model = model.eval()\n",
    "        losses = []\n",
    "        correct_predictions = 0\n",
    "        with torch.no_grad():\n",
    "            for data in tqdm(dev_data_loader):\n",
    "                data = [i.to(device) for i in data]\n",
    "                outputs = model(input_ids=data[0], attention_mask=data[1])\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                target = data[2]\n",
    "                loss = loss_fn(outputs, target)\n",
    "                correct_predictions += torch.sum(preds == target)\n",
    "                losses.append(loss.item())\n",
    "                \n",
    "        dev_acc = correct_predictions.double() / len(dev_data)\n",
    "        dev_loss = np.mean(losses)\n",
    "    \n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_acc'].append(dev_acc)\n",
    "    history['val_loss'].append(dev_loss)\n",
    "    if dev_acc > best_accuracy:\n",
    "        print('Save Model')\n",
    "        torch.save(model.state_dict(), 'best_model_state.bin')\n",
    "        best_accuracy = dev_acc\n",
    "        print(dev_acc,train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据结构示例\n",
    "# DataFrame 包含以下列：['text_id', 'original_text', 'sentiment_polarity', 'opinion_words']\n",
    "data = {\n",
    "    \"text_id\": [1, 2],\n",
    "    \"original_text\": [\n",
    "        \"This product is absolutely amazing!\",\n",
    "        \"The service is terrible and disappointing.\"\n",
    "    ],\n",
    "    \"sentiment_polarity\": [\"positive\", \"neutral\"，\"negative\"],\n",
    "    \"opinion_words\": [\n",
    "        [\"amazing\", \"absolutely\"],\n",
    "        [\"terrible\", \"disappointing\"]\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 词性映射到 SentiWordNet 格式\n",
    "def get_swn_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return 'a'  # 形容词\n",
    "    elif tag.startswith('N'):\n",
    "        return 'n'  # 名词\n",
    "    elif tag.startswith('R'):\n",
    "        return 'r'  # 副词\n",
    "    elif tag.startswith('V'):\n",
    "        return 'v'  # 动词\n",
    "    return None\n",
    "\n",
    "# 计算情感强度\n",
    "def get_sentiment_intensity(word, pos):\n",
    "    try:\n",
    "        synsets = list(swn.senti_synsets(word, pos))\n",
    "        if synsets:\n",
    "            swn_scores = [synset.pos_score() - synset.neg_score() for synset in synsets]\n",
    "            return sum(swn_scores) / len(swn_scores)  # 平均分\n",
    "    except:\n",
    "        return 0  # 如果词未在 SentiWordNet 中找到\n",
    "    return 0\n",
    "\n",
    "# 将情感强度分类为5个类别\n",
    "def classify_intensity(intensity):\n",
    "    if intensity <= -0.6:\n",
    "        return \"very negative\"\n",
    "    elif -0.6 < intensity <= -0.2:\n",
    "        return \"negative\"\n",
    "    elif -0.2 < intensity <= 0.2:\n",
    "        return \"neutral\"\n",
    "    elif 0.2 < intensity <= 0.6:\n",
    "        return \"positive\"\n",
    "    else:\n",
    "        return \"very positive\"\n",
    "\n",
    "# 构建情感词典和分析强度\n",
    "def analyze_sentiment_intensity(df):\n",
    "    sentiment_dict = defaultdict(dict)\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        opinion_words = row['opinion_words']\n",
    "        tokenized_text = word_tokenize(row['original_text'])\n",
    "        pos_tags = pos_tag(tokenized_text)\n",
    "\n",
    "        for word in opinion_words:\n",
    "            pos = next((get_swn_pos(tag) for w, tag in pos_tags if w.lower() == word.lower()), None)\n",
    "            if pos:\n",
    "                intensity = get_sentiment_intensity(word, pos)\n",
    "                intensity_category = classify_intensity(intensity)\n",
    "                sentiment_dict[word]['intensity'] = intensity\n",
    "                sentiment_dict[word]['intensity_category'] = intensity_category\n",
    "                sentiment_dict[word]['polarity'] = row['sentiment_polarity']\n",
    "\n",
    "    return sentiment_dict\n",
    "\n",
    "# 调用函数\n",
    "sentiment_intensity_dict = analyze_sentiment_intensity(df)\n",
    "\n",
    "# 显示结果\n",
    "for word, details in sentiment_intensity_dict.items():\n",
    "    print(f\"Word: {word}, Intensity: {details['intensity']:.2f}, Intensity Category: {details['intensity_category']}, Polarity: {details['polarity']}\")\n",
    "\n",
    "# 将结果保存到 DataFrame\n",
    "results = [\n",
    "    {\"word\": word, \"intensity\": details['intensity'], \"intensity_category\": details['intensity_category'], \"polarity\": details['polarity']}\n",
    "    for word, details in sentiment_intensity_dict.items()\n",
    "]\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# 保存到 CSV\n",
    "results_df.to_csv(\"sentiment_intensity_results.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30301,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
